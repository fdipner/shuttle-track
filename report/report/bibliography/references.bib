@inproceedings{Sun2020,
   abstract = {TrackNet, a deep learning network, was proposed to track high-speed and tiny objects such as tennis balls and shuttlecocks from videos. To conquer low image quality issues such as blur, afterimage, and short-term occlusion, some number of consecutive images are input together to detect an flying object. In this work, TrackNetV2 is proposed to improve the performance of TrackNet from various aspects, especially processing speed, prediction accuracy, and GPU memory usage. First of all, the processing speed is improved from 2.6 FPS to 31.8 FPS. The performance boost is achieved by reducing the input image size and re-engineering the network from a Multiple-In Single-Out (MISO) design to a Multiple-In Multiple-Out (MIMO) design. Then, to improve the prediction accuracy, a comprehensive dataset from diverse badminton match videos is collected and labeled for training and testing. The dataset consists of 55563 frames from 18 badminton match videos. In addition, the network mechanisms are composed of not only VGG16 and upsampling layers but also U-net. Last, to reduce GPU memory usage, the data structure of the heatmap layer is remodeled from a pixel-wise one-hot encoding 3D array to a real-valued 2D array. To reflect the change of the heatmap representation, the loss function is redesigned from a RMSE-based function to a weighted cross-entropy based function. An overall validation shows that the accuracy, precision and recall of TrackNetV2 respectively reach 96.3%, 97.0% and 98.7% in the training phase and 85.2%, 97.2% and 85.4% in a test on a brand new match. The processing speed of the 3-in and 3-out version TrackNetV2 can reach 31.84 FPS. The dataset and source code of this work are available at https://nol.cs.nctu.edu.tw:234/open-source/TrackNetv2/.},
   author = {Nien En Sun and Yu Ching Lin and Shao Ping Chuang and Tzu Han Hsu and Dung Ru Yu and Ho Yi Chung and Tsi Ui Ik},
   doi = {10.1109/ICPAI51961.2020.00023},
   isbn = {9781665404839},
   booktitle = {Proceedings - 2020 International Conference on Pervasive Artificial Intelligence, ICPAI 2020},
   keywords = {Badminton,deep learning,heatmap,shuttlecock tracking},
   month = {12},
   pages = {86-91},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {TrackNetV2: Efficient Shuttlecock Tracking Network},
   year = {2020}
}
@inproceedings{Chen2023,
   abstract = {We present TrackNetV3, a sophisticated model designed to enhance the precision of shuttlecock localization in broadcast badminton videos. TrackNetV3 is composed of two core modules: trajectory prediction and rectification. The trajectory prediction module leverages an estimated background as auxiliary data to locate the shuttlecock in spite of the fluctuating visual interferences. This module also incorporates mixup data augmentation to formulate complex scenarios to strengthen the networkâ€™s robustness. Given that a shuttlecock can occasionally be obstructed, we create repair masks by analyzing the predicted trajectory, subsequently rectifying the path via inpainting. This process significantly enhances the accuracy of tracking and the completeness of the trajectory. Our experimental results illustrate a substantial enhancement over previous standard methods, increasing the accuracy from 87.72% to 97.51%. These results validate the effectiveness of TrackNetV3 in progressing shuttlecock tracking within the context of badminton matches. We release the source code at https://github.com/qaz812345/TrackNetV3.},
   author = {Yu Jou Chen and Yu Shuen Wang},
   doi = {10.1145/3595916.3626370},
   isbn = {9798400702051},
   booktitle = {Proceedings of the 5th ACM International Conference on Multimedia in Asia, MMAsia 2023},
   keywords = {Badminton,Shuttlecock tracking,trajectory rectification},
   month = {12},
   publisher = {Association for Computing Machinery, Inc},
   title = {TrackNetV3: Enhancing ShuttleCock Tracking with Augmentations and Trajectory Rectification},
   year = {2023}
}
@inproceedings{Raj2025,
   abstract = {Accurately detecting and tracking high-speed, small objects, such as balls in sports videos, is challenging due to factors like motion blur and occlusion. Although recent deep learning frameworks like TrackNetV1, V2, and V3 have advanced tennis ball and shuttlecock tracking, they often struggle in scenarios with partial occlusion or low visibility. This is primarily because these models rely heavily on visual features without explicitly incorporating motion information, which is crucial for precise tracking and trajectory prediction. In this paper, we introduce an enhancement to the TrackNet family by fusing high-level visual features with learnable motion attention maps through a motion-aware fusion mechanism, effectively emphasizing the moving ball's location and improving tracking performance. Our approach uses frame differencing maps, modulated by a motion prompt layer, to highlight key motion regions over time. Experimental results on the tennis ball and shuttlecock datasets show that our method enhances the tracking performance of both TrackNetV2 and V3. We refer to our lightweight, plug-and-play solution, built on top of the existing TrackNets, as TrackNetV4.},
   author = {Arjun Raj and Lei Wang and Tom Gedeon},
   doi = {10.1109/ICASSP49660.2025.10889364},
   isbn = {9798350368741},
   issn = {15206149},
   booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
   keywords = {fusion,motion attention,tracking},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps},
   year = {2025}
}
@article{Geetha2025,
   abstract = {Racquet sports like tennis, badminton and table tennis are popular worldwide. Players spend countless hours practising to perfect their skills and become the best in their domain. Players and coaches analyse multiple professional games to improve their own. To help with this, multiple deep-learning models have emerged that track the ball or shuttlecock in these games. However, they still face many challenges. We present the TrackNetV3 model in three different racquet sports - tennis, badminton, and table tennis. TrackNetV3 is a SOTA (State Of The Art) model used previously for shuttlecock trajectories. We aim to determine the tracking system's robustness by testing it on multiple sports, including scenarios with rapid movements and occlusions, which are common in racquet sports. The TrackNetV3 has been trained and tested on the 2017 Summer Universiade tennis tournament's men's singles championship, 18 broadcast videos which include professional and amateur gameplays for badminton and 12 broadcast videos from the OpenTT dataset for table tennis. We prove the model's robustness and performance through extensive experiments compared to TrackNetV2 and YOLOv9.},
   author = {S. Geetha and Narayanan Ganesh and A. Sheik Abdullah and Manvik Sreedath and Utkarsh Mishra},
   doi = {10.1016/J.PROCS.2025.04.566},
   issn = {1877-0509},
   journal = {Procedia Computer Science},
   keywords = {Badminton,Table Tennis,Tennis,TrackNetV2,TrackNetV3,YOLOv9},
   month = {1},
   pages = {3084-3094},
   publisher = {Elsevier},
   title = {High Speed and Tiny Objects Tracking System in Racquet Sports Videos Using Deep Learning with Trajectory Rectification Feature},
   volume = {258},
   url = {https://www.sciencedirect.com/science/article/pii/S1877050925016709},
   year = {2025}
}
@article{Huang2019,
   abstract = {Ball trajectory data are one of the most fundamental and useful information in the evaluation of players' performance and analysis of game strategies. Although vision-based object tracking techniques have been developed to analyze sport competition videos, it is still challenging to recognize and position a high-speed and tiny ball accurately. In this paper, we develop a deep learning network, called TrackNet, to track the tennis ball from broadcast videos in which the ball images are small, blurry, and sometimes with afterimage tracks or even invisible. The proposed heatmap-based deep learning network is trained to not only recognize the ball image from a single frame but also learn flying patterns from consecutive frames. TrackNet takes images with a size of $640\times360$ to generate a detection heatmap from either a single frame or several consecutive frames to position the ball and can achieve high precision even on public domain videos. The network is evaluated on the video of the men's singles final at the 2017 Summer Universiade, which is available on YouTube. The precision, recall, and F1-measure of TrackNet reach $99.7\%$, $97.3\%$, and $98.5\%$, respectively. To prevent overfitting, 9 additional videos are partially labeled together with a subset from the previous dataset to implement 10-fold cross-validation, and the precision, recall, and F1-measure are $95.3\%$, $75.7\%$, and $84.3\%$, respectively. A conventional image processing algorithm is also implemented to compare with TrackNet. Our experiments indicate that TrackNet outperforms conventional method by a big margin and achieves exceptional ball tracking performance. The dataset and demo video are available at https://nol.cs.nctu.edu.tw/ndo3je6av9/.},
   author = {Yu Chuan Huang and I. No Liao and Ching Hsuan Chen and Tsi Ui Ik and Wen Chih Peng},
   doi = {10.1109/AVSS.2019.8909871},
   isbn = {9781728109909},
   journal = {2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2019},
   keywords = {Index Terms-Deep Learning,badminton,heatmap,neural networks,tennis,tiny object tracking},
   month = {7},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {TrackNet: A Deep Learning Network for Tracking High-speed and Tiny Objects in Sports Applications},
   url = {https://arxiv.org/pdf/1907.03698},
   year = {2019}
}
@article{Cao2021,
   abstract = {The ability to identify objects of interest from digital visual signals is critical for many applications of intelligent systems. For such object detection task, accuracy and computational efficiency are two important aspects, especially for applications with real-time requirement. In this paper, we study shuttlecock detection problem of a badminton robot, which is very challenging since the shuttlecock often moves fast in complex contexts, and must be detected precisely in real time so that the robot can plan and execute its following movements. To this end, we propose two novel variants of Tiny YOLOv2, a well-known deep learning based detector. We first modify the loss function to adaptively improve the detection speed for small objects such as shuttlecock. We then modify the architecture of Tiny YOLOv2 to retain more semantic information of small objects, so as to further improve the performance. Experimental results show that the proposed networks can achieve high detection accuracy with the fastest speed, compared with state-of-the-art deep detectors such as Faster R-CNN, SSD, Tiny YOLOv2, and YOLOv3. Our methods could be potentially applied to other tasks of detecting high-speed small objects.},
   author = {Zhiguang Cao and Tingbo Liao and Wen Song and Zhenghua Chen and Chongshou Li},
   doi = {10.1016/J.ESWA.2020.113833},
   issn = {0957-4174},
   journal = {Expert Systems with Applications},
   keywords = {Badminton robot,Deep learning,Object detection,YOLO},
   month = {2},
   pages = {113833},
   publisher = {Pergamon},
   title = {Detecting the shuttlecock for a badminton robot: A YOLO based approach},
   volume = {164},
   url = {https://www.sciencedirect.com/science/article/pii/S0957417420306436},
   year = {2021}
}
@article{Zhang2020,
   abstract = {This paper focuses on the problem of online golf ball detection and tracking from image sequences. An efficient real-time approach is proposed by exploiting convolutional neural networks (CNN) based object detection and a Kalman filter based prediction. Five classical deep learning-based object detection networks are implemented and evaluated for ball detection, including YOLO v3 and its tiny version, YOLO v4, Faster R-CNN, SSD, and RefineDet. The detection is performed on small image patches instead of the entire image to increase the performance of small ball detection. At the tracking stage, a discrete Kalman filter is employed to predict the location of the ball and a small image patch is cropped based on the prediction. Then, the object detector is utilized to refine the location of the ball and update the parameters of Kalman filter. In order to train the detection models and test the tracking algorithm, a collection of golf ball dataset is created and annotated. Extensive comparative experiments are performed to demonstrate the effectiveness and superior tracking performance of the proposed scheme.},
   author = {Tianxiao Zhang and Xiaohan Zhang and Yiju Yang and Zongbo Wang and Guanghui Wang},
   month = {12},
   title = {Efficient Golf Ball Detection and Tracking Based on Convolutional Neural Networks and Kalman Filter},
   url = {https://arxiv.org/pdf/2012.09393},
   year = {2020}
}
@article{Lai2025,
   abstract = {The 3D trajectory of a shuttlecock required for a badminton rally robot for human-robot competition demands real-time performance with high accuracy. However, the fast flight speed of the shuttlecock, along with various visual effects, and its tendency to blend with environmental elements, such as court lines and lighting, present challenges for rapid and accurate 2D detection. In this paper, we first propose the YO-CSA detection network, which optimizes and reconfigures the YOLOv8s model's backbone, neck, and head by incorporating contextual and spatial attention mechanisms to enhance model's ability in extracting and integrating both global and local features. Next, we integrate three major subtasks, detection, prediction, and compensation, into a real-time 3D shuttlecock trajectory detection system. Specifically, our system maps the 2D coordinate sequence extracted by YO-CSA into 3D space using stereo vision, then predicts the future 3D coordinates based on historical information, and re-projects them onto the left and right views to update the position constraints for 2D detection. Additionally, our system includes a compensation module to fill in missing intermediate frames, ensuring a more complete trajectory. We conduct extensive experiments on our own dataset to evaluate both YO-CSA's performance and system effectiveness. Experimental results show that YO-CSA achieves a high accuracy of 90.43% mAP@0.75, surpassing both YOLOv8s and YOLO11s. Our system performs excellently, maintaining a speed of over 130 fps across 12 test sequences.},
   author = {Yuan Lai and Zhiwei Shi and Chengxi Zhu},
   month = {1},
   title = {YO-CSA-T: A Real-time Badminton Tracking System Utilizing YOLO Based on Contextual and Spatial Attention},
   url = {https://arxiv.org/pdf/2501.06472},
   year = {2025}
}
